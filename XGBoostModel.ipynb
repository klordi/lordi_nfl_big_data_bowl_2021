{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score\n",
    "import pandas as pd\n",
    "\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('data/EventPassArrives.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['nx-CB0', 'nx-CB1', 'nx-CB2', 'nx-CB3', 'nx-CB4', 'nx-DB0', 'nx-DB1',\n",
       "       'nx-DB2', 'nx-DB3', 'nx-DE0',\n",
       "       ...\n",
       "       'yardsToGo', 'down', 'route', 'defTeam', 'offenseFormation',\n",
       "       'typeDropback', 'defendersInTheBox', 'numberOfPassRushers',\n",
       "       'absoluteYardlineNumber', 'passResult'],\n",
       "      dtype='object', length=304)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.rename(columns={x:\"-\".join(list(x)) if type(x)==tuple else x for x in df.columns})\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.query('passResult>=0')[list(df.columns)[:-1]].values\n",
    "y = df.query('passResult>=0')[list(df.columns)[-1]].values\n",
    "#y = df.query('bdefense==-1')['ir01'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1, 2}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_train = xgb.DMatrix(X_train, label=Y_train)\n",
    "D_test = xgb.DMatrix(X_test, label=Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "    'eta': 0.3, \n",
    "    'max_depth': 6,  \n",
    "    'objective': 'multi:softprob',  \n",
    "    'num_class': 3} \n",
    "\n",
    "param1 = {\n",
    "    'eta': 0.3, \n",
    "    'max_depth': 6,  \n",
    "    'objective': 'binary:logistic',  \n",
    "    'num_class': 2} \n",
    "\n",
    "steps = 40  # The number of training iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xgb.train(param, D_train, steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preds from Test Data\n",
      "Precision = 0.46072672303510914\n",
      "Recall = 0.3922617337169445\n",
      "Accuracy = 0.7869381673702508\n",
      "Preds from Training Data\n",
      "Precision = 0.9589054805165428\n",
      "Recall = 0.7375624718435841\n",
      "Accuracy = 0.9025013304949441\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score\n",
    "\n",
    "preds = model.predict(D_test)\n",
    "preds1 = model.predict(D_train)\n",
    "best_preds = np.asarray([np.argmax(line) for line in preds])\n",
    "best_preds1 = np.asarray([np.argmax(line) for line in preds1])\n",
    "\n",
    "print(\"Preds from Test Data\")\n",
    "print(\"Precision = {}\".format(precision_score(Y_test, best_preds, average='macro')))\n",
    "print(\"Recall = {}\".format(recall_score(Y_test, best_preds, average='macro')))\n",
    "print(\"Accuracy = {}\".format(accuracy_score(Y_test, best_preds)))\n",
    "print(\"Preds from Training Data\")\n",
    "print(\"Precision = {}\".format(precision_score(Y_train, best_preds1, average='macro')))\n",
    "print(\"Recall = {}\".format(recall_score(Y_train, best_preds1, average='macro')))\n",
    "print(\"Accuracy = {}\".format(accuracy_score(Y_train, best_preds1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.91054827, 0.07873426, 0.01071744],\n",
       "       [0.31375512, 0.66936123, 0.01688357],\n",
       "       [0.9536082 , 0.04174539, 0.00464647],\n",
       "       ...,\n",
       "       [0.75522345, 0.21609108, 0.02868549],\n",
       "       [0.81618536, 0.18004557, 0.00376906],\n",
       "       [0.91769934, 0.0747347 , 0.00756599]], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
